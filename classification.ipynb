{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1f9abd9-2e8b-40fb-995a-623d46790162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # 디버그 메시지 끄기\n",
    "\n",
    "import tensorflow as tf\n",
    "# gpu 사용 확인\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from angle_out import out\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, LSTM, Dense, Dropout, Embedding, Input\n",
    "from keras import layers\n",
    "import yaml\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import natsort\n",
    "import matplotlib.pyplot as plt  # 데이터 시각화\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def make_random_forest(\n",
    "    n_estimators=100, max_depth=10, min_samples_split=2, random_state=0, verbose=1\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=random_state,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_xgboost(verbose=2):\n",
    "    model = xgb.XGBClassifier(verbosity=verbose)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_lstm():\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        layers.LSTM(128, activation=\"relu\", input_shape=(10, 14), return_sequences=True)\n",
    "    )\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.LSTM(128, activation=\"relu\", return_sequences=False))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    # model.add(layers.TimeDistributed(layers.Dense(13, activation=\"softmax\")))\n",
    "    model.add(layers.Dense(13, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_gru():\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        layers.GRU(\n",
    "            128,\n",
    "            activation=\"sigmoid\",\n",
    "            input_shape=(10, 14),\n",
    "            dropout=0.2,\n",
    "            return_sequences=False,\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(13, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"label/BODYLOWER/20201024_dog-bodylower-000061.npy\")\n",
    "print(data[0].shape)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be4b1f67-711d-4463-8651-e9354d772892",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./label/BODYLOWER > \n",
      "./label/BODYSCRATCH > \n",
      "./label/BODYSHAKE > \n",
      "./label/FEETUP > \n",
      "./label/FOOTUP > \n",
      "./label/HEADING > \n",
      "./label/LYING > \n",
      "./label/MOUNTING > \n",
      "./label/SIT > \n",
      "./label/TAILING > \n",
      "./label/TAILLOW > \n",
      "./label/TURN > \n",
      "./label/WALKRUN > \n",
      "data shape > 164557\n",
      "label shape > (164557, 1)\n",
      "label list > ['BODYLOWER', 'BODYSCRATCH', 'BODYSHAKE', 'FEETUP', 'FOOTUP', 'HEADING', 'LYING', 'MOUNTING', 'SIT', 'TAILING', 'TAILLOW', 'TURN', 'WALKRUN']\n",
      "x shape > (164557, 10, 14)\n",
      "y tmp shape > (164557, 1)\n",
      "y one hot shape > (164557, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pieroot/miniconda3/envs/dlc/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# meta_data = []\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "window = 10\n",
    "\n",
    "path = \"./label\"\n",
    "label_path = \"./label\"\n",
    "\n",
    "with open(\"Zoo/config.yaml\") as f:\n",
    "    conf_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    bodyparts = []\n",
    "    for part in conf_yaml[\"bodyparts\"]:\n",
    "        bodyparts.append(part)\n",
    "\n",
    "\n",
    "label_parts = [\n",
    "    [\"nose\", \"neck_base\", \"neck_end\"],\n",
    "    [\"neck_base\", \"neck_end\", \"back_base\"],\n",
    "    [\"neck_end\", \"back_base\", \"back_middle\"],\n",
    "    [\"back_base\", \"back_middle\", \"back_end\"],\n",
    "    [\"back_middle\", \"back_end\", \"tail_base\"],\n",
    "    [\"back_end\", \"tail_base\", \"tail_end\"],\n",
    "    [\"back_base\", \"front_left_thai\", \"front_left_knee\"],\n",
    "    [\"front_left_thai\", \"front_left_knee\", \"front_left_paw\"],\n",
    "    [\"back_base\", \"front_right_thai\", \"front_right_knee\"],\n",
    "    [\"front_right_thai\", \"front_right_knee\", \"front_right_paw\"],\n",
    "    [\"back_end\", \"back_left_thai\", \"back_left_knee\"],\n",
    "    [\"back_left_thai\", \"back_left_knee\", \"back_left_paw\"],\n",
    "    [\"back_end\", \"back_right_thai\", \"back_right_knee\"],\n",
    "    [\"back_right_thai\", \"back_right_knee\", \"back_right_paw\"],\n",
    "]\n",
    "\n",
    "iii = 0\n",
    "\n",
    "# print(\"data\")\n",
    "label_list = []\n",
    "for list_ in natsort.natsorted(os.listdir(path)):\n",
    "    print(f\"{path}/{list_} > \")\n",
    "\n",
    "    delta = f\"{path}/{list_}\"\n",
    "    for delta_list in natsort.natsorted(os.listdir(delta)):\n",
    "        alpha = f\"{delta}/{delta_list}\"\n",
    "        if alpha.endswith(\".csv\"):\n",
    "            continue\n",
    "        elif alpha.endswith(\".npy\"):\n",
    "            # print(f\"{alpha} > \")\n",
    "            jeta = np.load(alpha, allow_pickle=True)\n",
    "            for index in range(jeta.shape[0]):\n",
    "                # print(jeta[index].shape)\n",
    "                if jeta[index].shape != (10, 14):\n",
    "                    # print(jeta[index].shape)\n",
    "                    # print(alpha)\n",
    "                    continue\n",
    "                data.append(jeta[index])\n",
    "                if not list_ in label_list:\n",
    "                    label_list.append(list_)\n",
    "                # label.append([label_list.index(list_)])\n",
    "                label.append([list_])\n",
    "\n",
    "            pass\n",
    "        iii += 1\n",
    "# # print(np.shape(data))\n",
    "print(f\"data shape > {len(data)}\")\n",
    "x_tmp = np.array(data, dtype=np.float32) / 180.0\n",
    "# # x_train = x_train.reshape(-1,4,10)\n",
    "# print(f\"shape > {y_tmp.shape}\")\n",
    "print(f\"label shape > {np.shape(label)}\")\n",
    "print(f\"label list > {label_list}\")\n",
    "y_tmp = np.array(label)\n",
    "# y_tmp = tf.one_hot(y_tmp, len(label_list))\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# encoder = OrdinalEncoder()\n",
    "y_one_hot = encoder.fit_transform(y_tmp)\n",
    "# y_one_hot = encoder.transform(y_tmp).toarray()\n",
    "\n",
    "pickle.dump(encoder, open(\"model/encoder.pkl\", \"wb\"))\n",
    "with open(\"model/label.json\", \"w\") as f:\n",
    "    json.dump(label_list, f)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, 13)\n",
    "\n",
    "\n",
    "# x_tmp = x_tmp.reshape(-1, x_tmp.shape[1], x_tmp.shape[2])\n",
    "# print(f\"1 > {x_tmp[69520]}\")\n",
    "\n",
    "print(f\"x shape > {x_tmp.shape}\")\n",
    "print(f\"y tmp shape > {y_tmp.shape}\")\n",
    "print(f\"y one hot shape > {y_one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78007dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tmp : [[ 1.74273056e+02 -6.61400070e+01 -1.91315735e+02  2.49659805e+01\n",
      "   1.23338715e+02 -1.72189606e+02 -8.82498322e+01  5.57902832e+01\n",
      "   2.10278244e+02 -2.39505875e+02  2.39476181e+02 -1.71361343e+02\n",
      "   9.72429733e+01 -2.06787445e+02]\n",
      " [ 1.83927765e+02 -1.92780014e+02  1.41493317e+02  8.99997482e+01\n",
      "  -2.51567581e+02  5.61558723e+01 -9.88318939e+01  8.97968197e+00\n",
      "  -3.08014404e+02  2.33268890e+02  6.39648972e+01 -1.76758560e+02\n",
      "  -1.62647228e+01  1.75269928e+02]\n",
      " [-9.55838470e+01  1.62709930e+02  1.63728638e+02 -7.49437027e+01\n",
      "  -1.85835083e+02  5.54859161e+01  5.50701790e+01 -2.66508389e+01\n",
      "   2.78239075e+02  2.56295090e+01  1.41971436e+02 -1.70454056e+02\n",
      "  -8.80357437e+01  1.72771820e+02]\n",
      " [-3.20902023e+01  1.77211868e+02 -2.29433594e+02  2.21534088e+02\n",
      "  -1.83918594e+02  6.55582199e+01  1.21866241e-01  1.11279039e+01\n",
      "  -5.77022095e+01  2.77058319e+02  1.64837769e+02 -2.69937805e+02\n",
      "   1.48875885e+02 -1.56362595e+02]\n",
      " [ 1.87684692e+02 -1.49975418e+02 -1.03884415e+02  2.05037247e+02\n",
      "  -1.81445557e+02  6.82805099e+01 -2.69239540e+01 -3.06243954e+01\n",
      "   2.38785004e+02  2.64308052e+01 -5.37553635e+01  1.79987091e+02\n",
      "   1.19507057e+02  1.20906670e+02]\n",
      " [-7.62221527e+01  2.31331879e+02 -2.22167389e+02  1.22320457e+02\n",
      "   5.82191277e+01 -1.93263657e+02  1.35024368e+02 -2.08616638e+02\n",
      "   2.49286530e+02 -4.06964731e+00 -2.01936752e+02  1.01956062e+01\n",
      "  -1.08874693e+01 -1.95854309e+02]\n",
      " [-9.97676468e+01  2.79356689e+02 -3.87579842e+01  9.89344711e+01\n",
      "  -2.71704987e+02  7.30348129e+01  2.57443909e+02 -2.09460785e+02\n",
      "  -1.77769211e+02  1.29336517e+02  1.06068817e+02 -1.81760834e+02\n",
      "   1.37864532e+02 -1.31530121e+02]\n",
      " [ 1.70924103e+02 -1.91204742e+02  5.54060173e+01  1.81149475e+02\n",
      "  -2.41528473e+02  3.93125687e+01  2.17412018e+02 -2.69659851e+02\n",
      "   8.36122971e+01  1.98793747e+02  1.62015442e+02 -2.21045410e+02\n",
      "  -1.03454056e+02  2.66149261e+02]\n",
      " [ 3.66555557e+01 -1.86144897e+02  7.52922893e-01  2.11497299e+02\n",
      "  -1.36616974e+02  5.58405075e+01 -2.04835907e+02 -8.96905594e+01\n",
      "  -1.91971588e+02 -1.97000237e+01 -1.64933578e+02  2.24970322e+02\n",
      "   1.45895447e+02 -1.72830536e+02]\n",
      " [-2.66167023e+02  1.47198837e+02 -1.45735352e+02  3.02491699e+02\n",
      "  -1.35294571e+02 -1.18074097e+02  4.87346992e+01  1.84636612e+02\n",
      "   2.06871201e+02 -2.45980225e+02  1.06586288e+02 -2.23251007e+02\n",
      "   1.78522766e+02 -1.61398636e+02]]\n",
      "y_tmp : ['BODYLOWER']\n",
      "y_one_hot : [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_recor : [['BODYLOWER']]\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_tmp : {x_tmp[0]}\")\n",
    "print(f\"y_tmp : {y_tmp[0]}\")\n",
    "print(f\"y_one_hot : {y_one_hot[0]}\")\n",
    "\n",
    "# y_recor = np.argmax(y_one_hot, axis=1).reshape(-1)\n",
    "# print([y_one_hot[100000]])\n",
    "y_record = encoder.inverse_transform([y_one_hot[0]])\n",
    "print(f\"y_recor : {y_record}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c8af3d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape > [[ 174.27306  -66.14001 -191.31573 ... -223.251    178.52277 -161.39864]\n",
      " [ 174.27306  -66.14001 -191.31573 ... -223.251    178.52277 -161.39864]\n",
      " [ 174.27306  -66.14001 -191.31573 ... -223.251    178.52277 -161.39864]\n",
      " ...\n",
      " [ 174.27306  -66.14001 -191.31573 ... -223.251    178.52277 -161.39864]\n",
      " [ 174.27306  -66.14001 -191.31573 ... -223.251    178.52277 -161.39864]\n",
      " [ 174.27306  -66.14001 -191.31573 ... -223.251    178.52277 -161.39864]]\n",
      "y list > ['BODYLOWER', 'BODYSCRATCH', 'BODYSHAKE', 'FEETUP', 'FOOTUP', 'HEADING', 'LYING', 'MOUNTING', 'SIT', 'TAILING', 'TAILLOW', 'TURN', 'WALKRUN']\n"
     ]
    }
   ],
   "source": [
    "# for i in y_tmp:\n",
    "#     if True in np.isnan(i):\n",
    "#         print('None')\n",
    "#         break\n",
    "\n",
    "# for i in x_tmp:\n",
    "#     if True in np.isnan(i):\n",
    "#         print('None')\n",
    "#         break\n",
    "x_train = x_tmp.reshape(-1, x_tmp.shape[1] * x_tmp.shape[2])\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "print(f\"x shape > {x_train[:10]}\")\n",
    "# print(f\"y shape > {y_tmp[1]}\")\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    # x_train[:10], y_tmp[:10], test_size=0.3, shuffle=True\n",
    "# )\n",
    "\n",
    "# print(f\"x train > {X_train.shape} | y train > {Y_train.shape}\")\n",
    "# print(f\"x test > {X_test.shape} | y test > {Y_test.shape}\")\n",
    "print(f\"y list > {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "075806f0-5175-473a-b6f4-4605705e4ad4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164557, 140) (164557, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   57.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 45, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 0, 'verbose': 1, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "def make_lstm():\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        layers.LSTM(128, activation=\"relu\", input_shape=(10, 14), return_sequences=True)\n",
    "    )\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.LSTM(128, activation=\"relu\", return_sequences=False))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    # model.add(layers.TimeDistributed(layers.Dense(13, activation=\"softmax\")))\n",
    "    model.add(layers.Dense(13, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_gru():\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        layers.GRU(\n",
    "            128,\n",
    "            activation=\"sigmoid\",\n",
    "            input_shape=(10, 14),\n",
    "            dropout=0.2,\n",
    "            return_sequences=False,\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(13, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "max_depth = 45\n",
    "n_estimators = 100\n",
    "\n",
    "\n",
    "def random_forest():\n",
    "    x_train = x_tmp.reshape(-1, x_tmp.shape[1] * x_tmp.shape[2])\n",
    "    model = make_random_forest(\n",
    "        max_depth=max_depth, n_estimators=n_estimators, random_state=0, verbose=1\n",
    "    )\n",
    "    print(x_train.shape, y_one_hot.shape)\n",
    "    model.fit(x_train, y_one_hot)\n",
    "    model_params = model.get_params()\n",
    "    print(model_params)\n",
    "    pickle.dump(model, open(f\"./model/rf_{max_depth}_{n_estimators}.pkl\", \"wb\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def lstm():\n",
    "    model = make_lstm()\n",
    "\n",
    "    # model.compile(loss='binary_crossentropy',\n",
    "    # optimizer='rmsprop',\n",
    "    # metrics=['accuracy'])\n",
    "    # model.compile(loss='mse', optimizer=Adam(0.01))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(x_tmp, y_one_hot, batch_size=64, epochs=100, verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def gru():\n",
    "    model = make_gru()\n",
    "\n",
    "    model.fit(x_tmp, y_one_hot, batch_size=64, epochs=40, verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def xgboost():\n",
    "    x_train = x_tmp.reshape(-1, x_tmp.shape[1] * x_tmp.shape[2])\n",
    "    model = make_xgboost()\n",
    "\n",
    "    model.fit(x_train, y_one_hot)\n",
    "    pickle.dump(model, open(\"./model/xgboost.pkl\", \"wb\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "# x_train=np.array(x_train) # (16,4,5,2) -> (16,4,10,1)\n",
    "\n",
    "# x_train = x_train.reshape(-1, x_train.shape[1],x_train.shape[2], 1)\n",
    "\n",
    "# print(x_train[0])\n",
    "# print(y_tmp[0])\n",
    "\n",
    "# model.fit(x_train, y_tmp, batch_size=1 , epochs=40, verbose=1)\n",
    "\n",
    "# model.save('lstm_model.h5')\n",
    "# model.save\n",
    "# print(\"모델 저장 완려\") #려~\n",
    "\n",
    "# pred = model.predict(x_train) # 테스트 데이터 예측\n",
    "\n",
    "# fig = plt.figure(facecolor='white')\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.plot(y_train, label='True')\n",
    "# ax.plot(pred, label='Prediction')\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "# print(x_tmp[1])\n",
    "model = random_forest()\n",
    "# model = lstm()\n",
    "# model = gru()\n",
    "# model = xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06a43e7c-a5b6-4efb-a241-e42565a02c38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score > 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy score > 1.0\n",
      "x_test[0] > [ 1.74273056e+02 -6.61400070e+01 -1.91315735e+02  2.49659805e+01\n",
      "  1.23338715e+02 -1.72189606e+02 -8.82498322e+01  5.57902832e+01\n",
      "  2.10278244e+02 -2.39505875e+02  2.39476181e+02 -1.71361343e+02\n",
      "  9.72429733e+01 -2.06787445e+02  1.83927765e+02 -1.92780014e+02\n",
      "  1.41493317e+02  8.99997482e+01 -2.51567581e+02  5.61558723e+01\n",
      " -9.88318939e+01  8.97968197e+00 -3.08014404e+02  2.33268890e+02\n",
      "  6.39648972e+01 -1.76758560e+02 -1.62647228e+01  1.75269928e+02\n",
      " -9.55838470e+01  1.62709930e+02  1.63728638e+02 -7.49437027e+01\n",
      " -1.85835083e+02  5.54859161e+01  5.50701790e+01 -2.66508389e+01\n",
      "  2.78239075e+02  2.56295090e+01  1.41971436e+02 -1.70454056e+02\n",
      " -8.80357437e+01  1.72771820e+02 -3.20902023e+01  1.77211868e+02\n",
      " -2.29433594e+02  2.21534088e+02 -1.83918594e+02  6.55582199e+01\n",
      "  1.21866241e-01  1.11279039e+01 -5.77022095e+01  2.77058319e+02\n",
      "  1.64837769e+02 -2.69937805e+02  1.48875885e+02 -1.56362595e+02\n",
      "  1.87684692e+02 -1.49975418e+02 -1.03884415e+02  2.05037247e+02\n",
      " -1.81445557e+02  6.82805099e+01 -2.69239540e+01 -3.06243954e+01\n",
      "  2.38785004e+02  2.64308052e+01 -5.37553635e+01  1.79987091e+02\n",
      "  1.19507057e+02  1.20906670e+02 -7.62221527e+01  2.31331879e+02\n",
      " -2.22167389e+02  1.22320457e+02  5.82191277e+01 -1.93263657e+02\n",
      "  1.35024368e+02 -2.08616638e+02  2.49286530e+02 -4.06964731e+00\n",
      " -2.01936752e+02  1.01956062e+01 -1.08874693e+01 -1.95854309e+02\n",
      " -9.97676468e+01  2.79356689e+02 -3.87579842e+01  9.89344711e+01\n",
      " -2.71704987e+02  7.30348129e+01  2.57443909e+02 -2.09460785e+02\n",
      " -1.77769211e+02  1.29336517e+02  1.06068817e+02 -1.81760834e+02\n",
      "  1.37864532e+02 -1.31530121e+02  1.70924103e+02 -1.91204742e+02\n",
      "  5.54060173e+01  1.81149475e+02 -2.41528473e+02  3.93125687e+01\n",
      "  2.17412018e+02 -2.69659851e+02  8.36122971e+01  1.98793747e+02\n",
      "  1.62015442e+02 -2.21045410e+02 -1.03454056e+02  2.66149261e+02\n",
      "  3.66555557e+01 -1.86144897e+02  7.52922893e-01  2.11497299e+02\n",
      " -1.36616974e+02  5.58405075e+01 -2.04835907e+02 -8.96905594e+01\n",
      " -1.91971588e+02 -1.97000237e+01 -1.64933578e+02  2.24970322e+02\n",
      "  1.45895447e+02 -1.72830536e+02 -2.66167023e+02  1.47198837e+02\n",
      " -1.45735352e+02  3.02491699e+02 -1.35294571e+02 -1.18074097e+02\n",
      "  4.87346992e+01  1.84636612e+02  2.06871201e+02 -2.45980225e+02\n",
      "  1.06586288e+02 -2.23251007e+02  1.78522766e+02 -1.61398636e+02]\n",
      "predict time > 0.011461734771728516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# with open(delta_label, \"r\") as label_json:\n",
    "# label_tmp = json.load(label_json)[0][\"pose\"]\n",
    "\n",
    "# model = keras.models.load_model('lstm_model.h5')\n",
    "# lenght = len(x_train)\n",
    "# faild = 0\n",
    "# fail_list = []\n",
    "# for i in tqdm(range(lenght), desc=f\"오차 {faild/lenght}\", mininterval=1):\n",
    "#     x_tmp = [x_train[i]]\n",
    "#     y_pred = model.predict(x_tmp)\n",
    "#     if y_pred != y_tmp[i]:\n",
    "#         faild += 1\n",
    "#         print(f\"실패 > {faild}/{i}\")\n",
    "#         fail_list.append(i)\n",
    "\n",
    "# print(f\"성공률 > {(1-faild/lenght)*100}\")\n",
    "\n",
    "\n",
    "def score_check():\n",
    "    x_test = x_tmp.reshape(-1, x_tmp.shape[1] * x_tmp.shape[2])\n",
    "    score = model.score(x_test, y_one_hot)\n",
    "    # model.evaluate(x_tmp, y_tmp)\n",
    "    print(f\"score > {score}\")\n",
    "    print(f\"model accuracy score > {accuracy_score(y_one_hot, model.predict(x_test))}\")\n",
    "\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"x_test[0] > {x_test[0]}\")\n",
    "    y_pred = model.predict([x_test[0]])\n",
    "    # model.\n",
    "    run_time = time.time() - start\n",
    "    print(f\"predict time > {run_time}\")\n",
    "\n",
    "    params = model.get_params()\n",
    "\n",
    "    params[\"score\"] = score\n",
    "    params[\"pred_time\"] = run_time\n",
    "    # json_data = json.dumps(params)\n",
    "    with open(f\"./model/depth_{max_depth}_{n_estimators}.json\", \"w\") as json_file:\n",
    "        json.dump(params, json_file)\n",
    "\n",
    "\n",
    "# print(f\"실패한 갯수 > {faild/len(x_train)}\")\n",
    "# print(f\"{x_train[0]}\")\n",
    "\n",
    "\n",
    "# y_pred = model.predict(x_tmp)\n",
    "# print(f\"예상치 > {y_pred}\")\n",
    "# print(f\"실제값 > {y_tmp[0]}\")\n",
    "\n",
    "score_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6feab874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140,)\n"
     ]
    }
   ],
   "source": [
    "x_test = x_tmp.reshape(-1, x_tmp.shape[1] * x_tmp.shape[2])\n",
    "\n",
    "print(np.shape(x_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
