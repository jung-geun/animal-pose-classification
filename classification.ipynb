{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f9abd9-2e8b-40fb-995a-623d46790162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 14:34:47.318299: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 디버그 메시지 끄기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# gpu 사용 확인\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # tf.config.experimental.set_visible_devices(gpus[0], \"GPU\")\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding, Dropout, GRU\n",
    "import matplotlib.pyplot as plt # 데이터 시각화\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import natsort\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from angle_out import out\n",
    "import numpy as np\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "def make_lstm():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(4, activation='relu', input_shape=(4, 1), return_sequences=True))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.LSTM(14, activation='relu', return_sequences=False))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_gru():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.GRU(4, activation='relu', input_shape=(4, 1), dropout = 0.2, return_sequences=True))\n",
    "    # model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.GRU(14, activation='relu', return_sequences=False))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_random_forest(n_estimators =100, max_depth=10, min_samples_split=2, random_state=0):\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=random_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4b1f67-711d-4463-8651-e9354d772892",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/BODYLOWER > \n",
      "./data/BODYSCRATCH > \n",
      "./data/BODYSHAKE > \n",
      "./data/FOOTUP > \n",
      "./data/HEADING > \n",
      "./data/LYING > \n",
      "./data/MOUNTING > \n",
      "./data/SIT > \n",
      "./data/TURN > \n",
      "data sample > [[-147.8194641 ]\n",
      " [ -36.73434893]\n",
      " [ -36.73434893]\n",
      " [  34.88673866]]\n",
      "x shape > (213130, 4, 1)\n",
      "y shape > (213130,)\n"
     ]
    }
   ],
   "source": [
    "# meta_data = []\n",
    "data = []\n",
    "label = []\n",
    "window = 4\n",
    "\n",
    "path = \"./data\"\n",
    "label_path = \"./label\"\n",
    "\n",
    "\n",
    "# body_parts = {\n",
    "#     '1' : '코',\n",
    "#     '2' : '이마 정 중앙',\n",
    "#     '3' : '입꼬리(입끝)',\n",
    "#     '4' : '아래 입술 중앙',\n",
    "#     '5' : '목',\n",
    "#     '6' : '앞다리 오른쪽 시작',\n",
    "#     '7' : '앞다리 왼쪽 시작',\n",
    "#     '8' : '앞다리 오른쪽 발목',\n",
    "#     '9' : '앞다리 왼쪽 발목',\n",
    "#     '10' : '오른쪽 대퇴골',\n",
    "#     '11' : '왼쪽 대퇴골',\n",
    "#     '12' : '뒷다리 오른쪽 발목',\n",
    "#     '13' : '뒷다리 왼쪽 발목',\n",
    "#     '14' : '꼬리 시작',\n",
    "#     '15' : '꼬리 끝',\n",
    "# }\n",
    "\n",
    "keypoint = [\n",
    "    \"nose\",\n",
    "    \"forehead\",\n",
    "    \"mouth\",\n",
    "    \"under_mouth\",\n",
    "    \"neck\",\n",
    "    \"right_front_start\",\n",
    "    \"left_front_start\",\n",
    "    \"right_front_ankle\",\n",
    "    \"left_front_ankle\",\n",
    "    \"right_thigh\",\n",
    "    \"left_thigh\",\n",
    "    \"right_back_ankle\",\n",
    "    \"left_back_ankle\",\n",
    "    \"tail_start\",\n",
    "    \"tail_end\"\n",
    "]\n",
    "\n",
    "# label_link = {\n",
    "#     '1' : [1, 2, 5, 14, 15],\n",
    "#     '2' : [5, 6, 8],\n",
    "#     '3' : [5, 7, 9],\n",
    "#     '4' : [14, 10, 12],\n",
    "#     '5' : [14, 11, 13],\n",
    "# }\n",
    "\n",
    "label_parts = [\n",
    "    [\"under_mouth\", \"nose\", \"forehead\", \"neck\"],\n",
    "    [\"nose\", \"forehead\", \"neck\", \"tail_start\"],\n",
    "    [\"forehead\", \"neck\", \"tail_start\", \"tail_end\"],\n",
    "    [\"forehead\", \"neck\", \"right_front_start\", \"right_front_ankle\"],\n",
    "    [\"forehead\", \"neck\", \"left_front_start\", \"left_front_ankle\"],\n",
    "    [\"neck\", \"tail_start\", \"right_thigh\", \"right_back_ankle\"],\n",
    "    [\"neck\", \"tail_start\", \"left_thigh\", \"left_back_ankle\"],\n",
    "]\n",
    "\n",
    "iii = 0\n",
    "\n",
    "# print(\"data\")\n",
    "\n",
    "for data_list in natsort.natsorted(os.listdir(path)):\n",
    "    if data_list == \"TEST\":\n",
    "        continue\n",
    "    elif \"_DLC\" in data_list:\n",
    "        continue \n",
    "    elif \"_bak\" in data_list:\n",
    "        continue\n",
    "    elif not os.path.isdir(f\"{path}/{data_list}\"):\n",
    "        continue\n",
    "    print(f\"{path}/{data_list} > \")\n",
    "        \n",
    "    delta = f\"{path}/{data_list}\"\n",
    "    for label_list in natsort.natsorted(os.listdir(delta)):\n",
    "        iii += 1\n",
    "        # _json_ = label_list\n",
    "        # delta_np = f\"{delta}/{delta_list}/{delta_list}.npy\"\n",
    "        delta_label = f\"{label_path}/{data_list}/{label_list}.json\"\n",
    "        # delta_label = f\"{delta}/{delta_list}/{delta_list}.json\"\n",
    "        \n",
    "        # print(f\"{delta_label} > \")\n",
    "        \n",
    "        keypoints = []\n",
    "        \n",
    "        if not os.path.exists(delta_label):\n",
    "            continue\n",
    "        \n",
    "        with open(delta_label, \"r\") as label_json:\n",
    "            label_tmp = json.load(label_json)\n",
    "            for annotation in label_tmp['annotations']:\n",
    "                # frame = annotation['frame_number']\n",
    "                # timestamp = annotation['timestamp']\n",
    "                key = annotation['keypoints']\n",
    "                # print(keypoint)\n",
    "                # if keypoint == None:\n",
    "                    # keypoint['x'] = 0\n",
    "                    # keypoint['y'] = 0\n",
    "                # print(annotation['keypoints'])\n",
    "                # if None in key:\n",
    "                # print(key)\n",
    "                for val in key:\n",
    "                    # print(val)\n",
    "                    if key[val] == None:\n",
    "                        key[val] = {'x' : 0 , 'y' : 0}\n",
    "                keypoints.append(key)\n",
    "                # print(keypoints) \n",
    "            # x_tmp = json.load(label_json)[\"annotations\"][][\"keypoints\"]\n",
    "            \n",
    "        # print(len(data))\n",
    "        # print(len(label))\n",
    "        # print(label_tmp)\n",
    "    \n",
    "                \n",
    "        # np_tmp = np.load(delta_np)\n",
    "        tmp = np.array(keypoints)\n",
    "        # print(tmp[0])\n",
    "        \n",
    "        # print(tmp[0].shape)\n",
    "        meta_data = []\n",
    "        # angle_arr = []\n",
    "        for data_index in tmp:\n",
    "            # print(data_index)\n",
    "            data_tmp = []\n",
    "            for index in data_index:\n",
    "                # print(data_index[index])\n",
    "                data_tmp.append(data_index[index])\n",
    "            # angle = out(inputs = data_index, keypoint= keypoint, label_parts = label_parts)\n",
    "            # angle_arr.append(angle)\n",
    "        meta_data.append(np.array(data_tmp))\n",
    "            # print(data_tmp)\n",
    "        \n",
    "        angle_arr = []\n",
    "        # print(meta_data)\n",
    "        for i in range(len(meta_data)):\n",
    "            angle = out(inputs = meta_data[i], keypoint= keypoint, label_parts = label_parts)\n",
    "            angle_arr.append(angle)\n",
    "        \n",
    "        # print(angle_arr)\n",
    "        # print(f\"window {window}\")\n",
    "        # print(f\"len {len(meta_data)}\")\n",
    "        # print(f\"data > {meta_data[0]}\")\n",
    "        # print(f\"angle > {angle_arr}\")\n",
    "        np_tmp = []\n",
    "        for index in range(window,len(angle_arr[0])):\n",
    "            tmp_wind = angle_arr[0][index-window:index]\n",
    "            \n",
    "            data.append(tmp_wind)\n",
    "            np_tmp.append(tmp_wind)\n",
    "            label.append(data_list)\n",
    "        \n",
    "        np_save = f\"{label_path}/{data_list}/{label_list}.npy\"\n",
    "        np.save(np_save, np_tmp)\n",
    "        # label.append(data_list)\n",
    "            \n",
    "            \n",
    "# print(f\"count = {iii}\")\n",
    "            \n",
    "# print(f\" {len(data)}\")\n",
    "# print(len(label))\n",
    "\n",
    "# angle_arr = []\n",
    "# data_t = []\n",
    "\n",
    "\n",
    "    # print(meta_data[index])\n",
    "    # print(label[index])\n",
    "    # for i in range(len(data[index])):\n",
    "    # print(data[index])\n",
    "        # angle = out(inputs = meta_data[index], keypoint= keypoint, label_parts = label_parts)\n",
    "        # data_t.append(angle)\n",
    "    \n",
    "    # for index in range(window,len(np_tmp)):\n",
    "    #     data.append(angle_arr[index-window:index])\n",
    "    #     label.append(label_tmp)\n",
    "    \n",
    "# print(f\"shape > {np.shape(data)}\")\n",
    "print(f\"data sample > {data[0]}\")\n",
    "        # angle_arr = []\n",
    "        # for i in range(len(np_tmp)):\n",
    "        #     angle = out(inputs = np_tmp[i])\n",
    "        #     angle_arr.append(angle)\n",
    "\n",
    "        #     for index in range(window,len(np_tmp)):\n",
    "        #         data.append(angle_arr[index-window:index])\n",
    "        #         label.append(label_tmp)\n",
    "\n",
    "# print(np.shape(data))\n",
    "x_train = np.array(data)\n",
    "# x_train = x_train.reshape(-1,4,10)\n",
    "y_train = np.array(label)\n",
    "\n",
    "print(f\"x shape > {x_train.shape}\")\n",
    "print(f\"y shape > {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0373161-10a5-425e-a30d-e79eb7eb1849",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213130,)\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {\"BODYLOWER\" : 0, \"BODYSCRATCH\" : 1, \"BODYSHAKE\" : 2, \"FOOTUP\" : 3, \"HEADING\" : 4, \"LYING\" : 5, \"MOUNTING\" : 6, \"SIT\" : 7, \"TURN\" : 8}\n",
    "\n",
    "def convert_word_to_index(word_to_index, sentences):\n",
    "    arr = []\n",
    "    for i in range(len(sentences)):\n",
    "        arr.append(word_to_index[sentences[i]])\n",
    "    arr = np.array(arr)\n",
    "    return arr\n",
    "    \n",
    "def one_hot_encoding(words, word_to_index):\n",
    "    ohv = []\n",
    "    for word in words:\n",
    "        one_hot_vector = [0]*(len(word_to_index))\n",
    "        index = word_to_index[word]\n",
    "        one_hot_vector[index] = 1\n",
    "        ohv.append([one_hot_vector])\n",
    "    ret = np.array(ohv)\n",
    "    return ret\n",
    "\n",
    "\n",
    "# y_tmp = tf.one_hot(y_train, 3, on_value=1.0, off_value=0.0)\n",
    "\n",
    "# y_tmp = one_hot_encoding(y_train, word_to_index)\n",
    "y_tmp = convert_word_to_index(word_to_index, y_train)\n",
    "# y_tmp = y_tmp.reshape(-1,1,3)\n",
    "\n",
    "# print(y_train.shape)\n",
    "print(y_tmp.shape)\n",
    "# print(x_train[0])\n",
    "# print(y_tmp[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "075806f0-5175-473a-b6f4-4605705e4ad4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 4, 4)              96        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4)              0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 14)                1064      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,205\n",
      "Trainable params: 1,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/40\n",
      "213130/213130 [==============================] - 1426s 7ms/step - loss: 23.3094\n",
      "Epoch 2/40\n",
      "213130/213130 [==============================] - 1426s 7ms/step - loss: 23.3093\n",
      "Epoch 3/40\n",
      "213130/213130 [==============================] - 1418s 7ms/step - loss: 23.3093\n",
      "Epoch 4/40\n",
      "213130/213130 [==============================] - 1405s 7ms/step - loss: 23.3093\n",
      "Epoch 5/40\n",
      "213130/213130 [==============================] - 1384s 6ms/step - loss: 23.3093\n",
      "Epoch 6/40\n",
      "213130/213130 [==============================] - 1385s 7ms/step - loss: 23.3093\n",
      "Epoch 7/40\n",
      "213130/213130 [==============================] - 1368s 6ms/step - loss: 23.3096\n",
      "Epoch 8/40\n",
      "213130/213130 [==============================] - 1379s 6ms/step - loss: 23.3093\n",
      "Epoch 9/40\n",
      " 83085/213130 [==========>...................] - ETA: 13:53 - loss: 23.3660"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 62\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m     39\u001b[0m \u001b[39m# x_train=np.array(x_train) # (16,4,5,2) -> (16,4,10,1)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# x_train = x_train.reshape(-1, x_train.shape[1],x_train.shape[2], 1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[39m# model = rf()\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m model \u001b[39m=\u001b[39m ls()\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mls\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mAdam(\u001b[39m0.01\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[0;32m---> 35\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_tmp, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m , epochs\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplabcut/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# np_data = np.load('./out/coco_train02.npy')\n",
    "# print(np_data.shape)\n",
    "# print(np_data[19])\n",
    "# print(data[0])\n",
    "\n",
    "# window = 4\n",
    "# arr =[]\n",
    " \n",
    "# model = lstm()\n",
    "max_depth = 45\n",
    "n_estimators = 25\n",
    "\n",
    "x_train = x_train.reshape(-1, 4)\n",
    "\n",
    "def random_forest():\n",
    "    model = make_random_forest(max_depth=max_depth, n_estimators=n_estimators, random_state=0)\n",
    "    print(x_train.shape, y_tmp.shape)\n",
    "    model.fit(x_train, y_tmp)\n",
    "    model_params = model.get_params()\n",
    "    print(model_params)\n",
    "    pickle.dump(model, open(f\"./out/rf_{max_depth}_{n_estimators}.pkl\", 'wb'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def lstm():\n",
    "    model = make_lstm()\n",
    "    \n",
    "    # model.compile(loss='binary_crossentropy',\n",
    "                # optimizer='rmsprop',\n",
    "                # metrics=['accuracy'])\n",
    "    # model.compile(loss='mse', optimizer=Adam(0.01))\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(x_train, y_tmp, batch_size=1 , epochs=40, verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "def gru():\n",
    "    model = make_gru()\n",
    "    \n",
    "    model.fit(x_train, y_tmp, batch_size=1 , epochs=40, verbose=1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# x_train=np.array(x_train) # (16,4,5,2) -> (16,4,10,1)\n",
    "\n",
    "# x_train = x_train.reshape(-1, x_train.shape[1],x_train.shape[2], 1)\n",
    "\n",
    "# print(x_train[0])\n",
    "# print(y_tmp[0])\n",
    "\n",
    "# model.fit(x_train, y_tmp, batch_size=1 , epochs=40, verbose=1)\n",
    "\n",
    "# model.save('lstm_model.h5')\n",
    "# model.save\n",
    "# print(\"모델 저장 완려\") #려~\n",
    "\n",
    "# pred = model.predict(x_train) # 테스트 데이터 예측\n",
    "\n",
    "# fig = plt.figure(facecolor='white')\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.plot(y_train, label='True')\n",
    "# ax.plot(pred, label='Prediction')\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "# model = random_forest()\n",
    "model = lstm()\n",
    "# model = gru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a43e7c-a5b6-4efb-a241-e42565a02c38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score > 0.9976680898981842\n",
      "model accuracy score > 0.9976680898981842\n",
      "predict time > 0.0018236637115478516\n"
     ]
    }
   ],
   "source": [
    "# with open(delta_label, \"r\") as label_json:\n",
    "    # label_tmp = json.load(label_json)[0][\"pose\"]\n",
    "    \n",
    "# model = keras.models.load_model('lstm_model.h5')\n",
    "# lenght = len(x_train)\n",
    "# faild = 0\n",
    "# fail_list = []\n",
    "# for i in tqdm(range(lenght), desc=f\"오차 {faild/lenght}\", mininterval=1):\n",
    "#     x_tmp = [x_train[i]]\n",
    "#     y_pred = model.predict(x_tmp)\n",
    "#     if y_pred != y_tmp[i]:\n",
    "#         faild += 1\n",
    "#         print(f\"실패 > {faild}/{i}\")\n",
    "#         fail_list.append(i)\n",
    "\n",
    "# print(f\"성공률 > {(1-faild/lenght)*100}\")\n",
    "\n",
    "def score_check():\n",
    "\n",
    "    score = model.score(x_train, y_tmp)\n",
    "\n",
    "    print(f\"score > {score}\")\n",
    "    print(f\"model accuracy score > {accuracy_score(y_tmp, model.predict(x_train))}\")\n",
    "\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = model.predict([x_train[0]])\n",
    "    # model.\n",
    "    run_time = time.time() - start\n",
    "    print(f\"predict time > {run_time}\")\n",
    "\n",
    "    params = model.get_params()\n",
    "\n",
    "    params[\"score\"] = score\n",
    "    params[\"pred_time\"] = run_time\n",
    "    # json_data = json.dumps(params)\n",
    "    with open(f\"./model/depth_{max_depth}_{n_estimators}.json\", \"w\") as json_file:\n",
    "        json.dump(params, json_file)\n",
    "# print(f\"실패한 갯수 > {faild/len(x_train)}\")\n",
    "# print(f\"{x_train[0]}\")\n",
    "\n",
    "\n",
    "# y_pred = model.predict(x_tmp)\n",
    "# print(f\"예상치 > {y_pred}\")\n",
    "# print(f\"실제값 > {y_tmp[0]}\")\n",
    "\n",
    "score_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feab874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
