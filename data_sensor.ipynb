{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9740a0-b5c1-46a8-a9b3-c5b833a23fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 17:14:30.309589: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow :  2.10.0\n",
      "gpus :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Loading DLC 2.3.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pieroot/miniconda3/envs/dlc/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pieroot/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # 디버그 메시지 끄기\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow : \", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(\"gpus : \", gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import deeplabcut\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import angle_out as angle\n",
    "from data_sensor import analyze_frames\n",
    "\n",
    "# 동영상을 입력으로하며, 학습된 모델을 이용하여 프레임 단위로 분석하고 예측한 관절 좌표를 생성 및 저장한다.\n",
    "\n",
    "# 이거 필요 없음\n",
    "\"\"\"\n",
    "----- param -----\n",
    "src_file_path : 분석할 동영상 파일\n",
    "config_path : 학습된 모델의 yaml 파일 = dlc_cfg\n",
    "cfg_path : DLC 프로젝트의 yaml 파일 = cfg\n",
    "----- pose_cfg.yaml에 추가 -----\n",
    "\n",
    "init_weights: \"/drive/samba/private_files/jupyter/DLC/dog1/dlc-models/iteration-0/dog1Mar24-trainset95shuffle1/train/snapshot-30000\"\n",
    "mean_pixel: [123.68, 116.779, 103.939]\n",
    "weight_decay: 0.0001\n",
    "pairwise_predict: False\n",
    "partaffinityfield_predict: False\n",
    "stride: 8.0\n",
    "intermediate_supervision: False\n",
    "dataset_type: imgaug\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" edit. 폴더명 파라미터로 받아서 실행하기\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Zoo/config.yaml\") as f:\n",
    "    conf_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    # print(conf_yaml[\"bodyparts\"])\n",
    "    bodyparts = []\n",
    "    for part in conf_yaml[\"bodyparts\"]:\n",
    "        bodyparts.append(part)\n",
    "\n",
    "\n",
    "label_parts = [\n",
    "    [\"nose\", \"neck_base\", \"neck_end\"],\n",
    "    [\"neck_base\", \"neck_end\", \"back_base\"],\n",
    "    [\"neck_end\", \"back_base\", \"back_middle\"],\n",
    "    [\"back_base\", \"back_middle\", \"back_end\"],\n",
    "    [\"back_middle\", \"back_end\", \"tail_base\"],\n",
    "    [\"back_end\", \"tail_base\", \"tail_end\"],\n",
    "    [\"back_base\", \"front_left_thai\", \"front_left_knee\"],\n",
    "    [\"front_left_thai\", \"front_left_knee\", \"front_left_paw\"],\n",
    "    [\"back_base\", \"front_right_thai\", \"front_right_knee\"],\n",
    "    [\"front_right_thai\", \"front_right_knee\", \"front_right_paw\"],\n",
    "    [\"back_end\", \"back_left_thai\", \"back_left_knee\"],\n",
    "    [\"back_left_thai\", \"back_left_knee\", \"back_left_paw\"],\n",
    "    [\"back_end\", \"back_right_thai\", \"back_right_knee\"],\n",
    "    [\"back_right_thai\", \"back_right_knee\", \"back_right_paw\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507aab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label > BODYLOWER:   0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label > BODYLOWER:  83%|████████▎ | 661/800 [46:13<09:43,  4.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m video_list \u001b[39m=\u001b[39m natsort\u001b[39m.\u001b[39mnatsorted(os\u001b[39m.\u001b[39mlistdir(video_dir))\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m video \u001b[39min\u001b[39;00m tqdm(video_list, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLabel > \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m     \u001b[39m# print(f\"{video_dir}/{video} | {label}\")\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     analyze_video(src\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mvideo_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mvideo\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, out\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36manalyze_video\u001b[0;34m(src, out)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# des_out = ./label1/WALKRUN/dog-walkrun-099432\u001b[39;00m\n\u001b[1;32m     14\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mlabel_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m data \u001b[39m=\u001b[39m analyze_frames(src, \u001b[39m10\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# print(data.shape[0])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m label \u001b[39m=\u001b[39m [label_ \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]\n",
      "File \u001b[0;32m/drive/samba/private_files/jupyter/DLC/data_sensor.py:213\u001b[0m, in \u001b[0;36manalyze_frames\u001b[0;34m(src_avi, window_size)\u001b[0m\n\u001b[1;32m    209\u001b[0m pos \u001b[39m=\u001b[39m get_img_coord(frame)\n\u001b[1;32m    211\u001b[0m \u001b[39m# print(f\"추가 한 후 {len(x_train)}\")\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m x_train\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    214\u001b[0m     angle\u001b[39m.\u001b[39mout(inputs\u001b[39m=\u001b[39mpos, body_parts\u001b[39m=\u001b[39mbodyparts, label_parts\u001b[39m=\u001b[39mlabel_parts)\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x_train) \u001b[39m>\u001b[39m window_size:\n\u001b[1;32m    217\u001b[0m     x_train\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/drive/samba/private_files/jupyter/DLC/data_sensor.py:155\u001b[0m, in \u001b[0;36mget_img_coord\u001b[0;34m(src_img)\u001b[0m\n\u001b[1;32m    150\u001b[0m     batch_ind \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m batch_ind \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    153\u001b[0m     \u001b[39m# take care of the last frames (the batch that might have been\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[39m# processed)\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     pose \u001b[39m=\u001b[39m predict\u001b[39m.\u001b[39;49mgetposeNP(\n\u001b[1;32m    156\u001b[0m         frames, dlc_cfg, sess, inputs, outputs\n\u001b[1;32m    157\u001b[0m     )  \u001b[39m# process the whole batch (some frames might be from previous batch!)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# print(\"getPose : \",type(pose),pose) # <class 'numpy.ndarray'> [[\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# 1.45855195e+02 6.34100614e+01 7.33174324e-01 2.44310002e+02 ...]..]\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(pose[\u001b[39m0\u001b[39m])  \u001b[39m# 63 (point_21 * values_3)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict.py:168\u001b[0m, in \u001b[0;36mgetposeNP\u001b[0;34m(image, cfg, sess, inputs, outputs, outall)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adapted from DeeperCut, performs numpy-based faster inference on batches.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mIntroduced in https://www.biorxiv.org/content/10.1101/457242v1\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m num_outputs \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_outputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 168\u001b[0m outputs_np \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39;49mrun(outputs, feed_dict\u001b[39m=\u001b[39;49m{inputs: image})\n\u001b[1;32m    170\u001b[0m scmap, locref \u001b[39m=\u001b[39m extract_cnn_outputmulti(outputs_np, cfg)  \u001b[39m# processes image batch.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m batchsize, ny, nx, num_joints \u001b[39m=\u001b[39m scmap\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1372\u001b[0m                        run_metadata)\n\u001b[1;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlc/lib/python3.8/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[1;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import natsort\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "def analyze_video(src=None, out=\"out\"):\n",
    "    # src = videos/WALKRUN_video/dog-walkrun-099432.mp4\n",
    "    label_ = src.split(\"_\")[0].split(\"/\")[-1]\n",
    "    # label_ = WALKRUN\n",
    "    out_postfix = src.split(\"/\")[-1].split(\".\")[0]\n",
    "    # out_postfix = dog-walkrun-099432\n",
    "    des_out = f\"{out}/{label_}/{out_postfix}\"\n",
    "    # des_out = ./label1/WALKRUN/dog-walkrun-099432\n",
    "    os.makedirs(f\"{out}/{label_}\", exist_ok=True)\n",
    "\n",
    "    data = analyze_frames(src, 10)\n",
    "    # print(data.shape[0])\n",
    "    label = [label_ for i in range(data.shape[0])]\n",
    "\n",
    "    np.save(f\"{des_out}.npy\", data)\n",
    "    with open(f\"{des_out}.csv\", \"w\") as f:\n",
    "        f.write(f\"{label}\\n\")\n",
    "    return data, label\n",
    "\n",
    "\n",
    "path = \"videos\"\n",
    "\n",
    "for label_dir in natsort.natsorted(os.listdir(path)):\n",
    "    video_dir = f\"{path}/{label_dir}\"\n",
    "    if os.path.isdir(video_dir) == False:\n",
    "        continue\n",
    "    label = label_dir.split(\"_\")[0]\n",
    "    # print(f\"{video_dir} | {label}\")\n",
    "    video_list = natsort.natsorted(os.listdir(video_dir))\n",
    "    for video in tqdm(video_list, desc=f\"Label > {label}\", leave=True):\n",
    "        # print(f\"{video_dir}/{video} | {label}\")\n",
    "        analyze_video(src=f\"{video_dir}/{video}\", out=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from data_sensor import analyze_frame\n",
    "\n",
    "data = analyze_frame(\"videos2/kakaotalk_1679886104324.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd972e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = pd.DataFrame()\n",
    "for i in range(len(data)):\n",
    "    p_data[f\"{i}_x\"] = data[i][:, 0]\n",
    "    p_data[f\"{i}_y\"] = data[i][:, 1]\n",
    "    p_data[f\"{i}_likelihood\"] = data[i][:, 2]\n",
    "p_data.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e628570",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = pd.read_csv(\"test.csv\")\n",
    "index = 0\n",
    "for i in p_data.iloc[0]:\n",
    "    if index % 3 == 2:\n",
    "        print(i)\n",
    "    # print(i)\n",
    "    index += 1\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945238d7-ea0a-4478-83c0-f1f592609655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_video(src=None, out=\"out\"):\n",
    "    # src = videos/WALKRUN_video/dog-walkrun-099432.mp4\n",
    "    # label_ = src.split(\"_\")[0].split(\"/\")[-1]\n",
    "    # label_ = WALKRUN\n",
    "    # out_postfix = src.split(\"/\")[-1].split(\".\")[0]\n",
    "    # out_postfix = dog-walkrun-099432\n",
    "    # des_out = f\"{out}/{label_}/{out_postfix}\"\n",
    "    # des_out = ./label1/WALKRUN/dog-walkrun-099432\n",
    "    # os.makedirs(f\"{out}/{label_}\", exist_ok=True)\n",
    "\n",
    "    data = analyze_frames(src, 10)\n",
    "    # print(data.shape[0])\n",
    "    label = [label_ for i in range(data.shape[0])]\n",
    "\n",
    "    # np.save(f\"{des_out}.npy\", data)\n",
    "    # with open(f\"{des_out}.csv\", \"w\") as f:\n",
    "        # f.write(f\"{label}\\n\")\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193cbdb-29cc-41fa-83dc-f247fe7b8d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyze_video(\n",
    "    src=\"videos/BODYLOWER_video/20201024_dog-bodylower-000061.mp4.mp4\", out=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150a090-1ee2-4c1b-88d6-30c29e88473e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # analyze_video(src ='./data/' + i + j1 , out='./label/' + i+'/' + j1)\n",
    "# data_path = './data'\n",
    "# label_path = './label2'\n",
    "# for i in os.listdir(data_path):\n",
    "#     if os.path.isdir(f'{data_path}/{i}') == False:\n",
    "#         continue\n",
    "#     if not os.path.exists(f'{label_path}/{i}'):\n",
    "#         os.makedirs(f'{label_path}/{i}')  # label 폴더에 행동 폴더 만들기\n",
    "\n",
    "#     for j in os.listdir(f'{data_path}/{i}'):\n",
    "#         # print(f\"./label/{i}/{j}\")\n",
    "#         f\"{data_path}/{i}/{j}\"\n",
    "#         # print(j)\n",
    "#         if not os.path.exists(f\"{label_path}/{i}/{j}\"):\n",
    "#             os.makedirs(f\"{label_path}/{i}/{j}\")  # label 폴더안에 행동 폴더 안에 데이터랑 이름이 같은 폴더 만들기\n",
    "\n",
    "#         f = open(f\"{label_path}/{i}/{j}/{j}.json\", \"w\")\n",
    "#         data = []\n",
    "#         data.append({\"frame\": 1, \"pose\": f\"{i}\"})\n",
    "#         json.dump(data, f, indent=4)\n",
    "#         for x in os.listdir(f\"{data_path}/{i}/{j}\"):\n",
    "#             print(f\"{data_path}/{i}/{j}/{x}\")\n",
    "#             analyze_video(src=f\"{data_path}/{i}/{j}/{x}\", out=label_path)\n",
    "#         f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
